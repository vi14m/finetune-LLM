{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMyCD4mlraVODfzM3/0FosY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vi14m/finetune-LLM/blob/main/finetuned_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ],
      "metadata": {
        "id": "i-GMXyswZ1XW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastModel\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import TextStreamer\n",
        "from unsloth.chat_templates import get_chat_template, standardize_data_formats, train_on_responses_only\n",
        "from trl import SFTTrainer, SFTConfig"
      ],
      "metadata": {
        "id": "A8DCPTbCdUZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name = \"unsloth/gemma-3-4b-it\",\n",
        "    max_seq_length = 2048,\n",
        "    load_in_4bit = True,\n",
        "    load_in_8bit = False,\n",
        "    full_finetuning = False,\n",
        ")"
      ],
      "metadata": {
        "id": "VpGBUq3ldYLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare LoRA model\n",
        "model = FastModel.get_peft_model(\n",
        "    model,\n",
        "    finetune_vision_layers = False,\n",
        "    finetune_language_layers = True,\n",
        "    finetune_attention_modules = True,\n",
        "    finetune_mlp_modules = True,\n",
        "    r = 8,\n",
        "    lora_alpha = 8,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    random_state = 3407,\n",
        ")"
      ],
      "metadata": {
        "id": "NuIEldkjdY1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set chat template\n",
        "tokenizer = get_chat_template(tokenizer, chat_template = \"gemma-3\")"
      ],
      "metadata": {
        "id": "PZmnAUiFdexS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load train, validation, and test datasets\n",
        "train_dataset = load_dataset(\"mbpp\", split = \"train\")\n",
        "val_dataset = load_dataset(\"mbpp\", split = \"validation\")\n",
        "test_dataset = load_dataset(\"mbpp\", split = \"test\")"
      ],
      "metadata": {
        "id": "e7vu9Fr-diCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def formatting_prompts_func(examples):\n",
        "    # Format MBPP dataset into conversation format suitable for Gemma-3\n",
        "    convos = []\n",
        "    for prompt_text, code, test_cases in zip(examples[\"text\"], examples[\"code\"], examples[\"test_list\"]):\n",
        "        assistant_response = code + \"\\n\\nTest Cases:\\n\" + \"\\n\".join(test_cases)\n",
        "        convo = [\n",
        "            {\"role\": \"user\", \"content\": prompt_text},\n",
        "            {\"role\": \"assistant\", \"content\": assistant_response}\n",
        "        ]\n",
        "        convos.append(convo)\n",
        "\n",
        "    # Apply the chat template and remove the leading <bos> token\n",
        "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False).removeprefix('<bos>') for convo in convos]\n",
        "    return { \"text\" : texts }\n",
        "\n",
        "# Format all datasets\n",
        "train_dataset = train_dataset.map(formatting_prompts_func, batched = True)\n",
        "val_dataset = val_dataset.map(formatting_prompts_func, batched = True)\n",
        "test_dataset = test_dataset.map(formatting_prompts_func, batched = True)"
      ],
      "metadata": {
        "id": "Xqg0DMjIdjKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = val_dataset,  # Using validation dataset for evaluation\n",
        "    args = SFTConfig(\n",
        "        dataset_text_field = \"text\",\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 40,\n",
        "        learning_rate = 2e-4,\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        report_to = \"none\",\n",
        "        eval_steps = 5,  # Evaluate every 10 steps\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "JB_TUwKpdi9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train only on responses\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part = \"<start_of_turn>user\\n\",\n",
        "    response_part = \"<start_of_turn>model\\n\",\n",
        ")"
      ],
      "metadata": {
        "id": "2ZyLAUETdh4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "cqMGiDoDdhwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_code(row, model, tokenizer):\n",
        "    \"\"\"Generates code for a given dataset row using the fine-tuned model.\"\"\"\n",
        "    prompt_text = row[\"text\"]\n",
        "    messages = [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt_text}]}]\n",
        "\n",
        "    # Apply chat template and add generation prompt\n",
        "    text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
        "\n",
        "    # Generate response from the model\n",
        "    outputs = model.generate(\n",
        "        **tokenizer([text], return_tensors=\"pt\").to(\"cuda\"),\n",
        "        max_new_tokens=256,\n",
        "        temperature=1.0,\n",
        "        top_p=0.95,\n",
        "        top_k=64,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # Decode and extract the generated content\n",
        "    decoded_output = tokenizer.batch_decode(outputs)[0]\n",
        "\n",
        "    try:\n",
        "        start_model_token = \"<start_of_turn>model\\n\"\n",
        "        end_model_token = \"<end_of_turn>\\n\"\n",
        "        start_index = decoded_output.find(start_model_token) + len(start_model_token)\n",
        "        end_index = decoded_output.rfind(end_model_token)\n",
        "        generated_content = decoded_output[start_index:end_index]\n",
        "\n",
        "        # Split to get code part before \"Test Cases:\"\n",
        "        test_cases_marker = \"\\n\\nTest Cases:\\n\"\n",
        "        code_end_index = generated_content.find(test_cases_marker)\n",
        "        if code_end_index != -1:\n",
        "            generated_code = generated_content[:code_end_index].strip()\n",
        "        else:\n",
        "            generated_code = generated_content.strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting generated code: {e}\")\n",
        "        generated_code = \"\"\n",
        "\n",
        "    return generated_code\n",
        "\n",
        "def evaluate_code(generated_code, test_cases, test_setup_code):\n",
        "    \"\"\"Evaluates the generated code against the provided test cases.\"\"\"\n",
        "    exec_globals = {}\n",
        "    try:\n",
        "        # Execute test setup code\n",
        "        if test_setup_code:\n",
        "            exec(test_setup_code, exec_globals)\n",
        "\n",
        "        # Execute the generated code\n",
        "        exec(generated_code, exec_globals)\n",
        "\n",
        "        # Execute test cases\n",
        "        all_tests_passed = True\n",
        "        for test in test_cases:\n",
        "            try:\n",
        "                exec(test, exec_globals)\n",
        "            except AssertionError:\n",
        "                all_tests_passed = False\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(f\"Error executing test case '{test}': {e}\")\n",
        "                all_tests_passed = False\n",
        "                break\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during code execution: {e}\")\n",
        "        all_tests_passed = False\n",
        "\n",
        "    return all_tests_passed"
      ],
      "metadata": {
        "id": "JJJqpxDfdxNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load original test data for evaluation\n",
        "test_dataset_original = load_dataset(\"mbpp\", split = \"test\")\n",
        "\n",
        "# Evaluate on a subset of the test dataset\n",
        "passed_count = 0\n",
        "total_problems = 10  # Evaluate on 10 problems for demonstration\n",
        "\n",
        "for i in range(total_problems):\n",
        "    print(f\"\\nEvaluating problem {i+1}/{total_problems}\")\n",
        "\n",
        "    # Use the formatted text for inference\n",
        "    formatted_row = test_dataset[i]\n",
        "    generated_code = generate_code(formatted_row, model, tokenizer)\n",
        "    print(f\"Generated code:\\n{generated_code}\")\n",
        "\n",
        "    # Use the original data for evaluation\n",
        "    original_row = test_dataset_original[i]\n",
        "    test_cases = original_row[\"test_list\"]\n",
        "    test_setup_code = original_row[\"test_setup_code\"]\n",
        "    if evaluate_code(generated_code, test_cases, test_setup_code):\n",
        "        passed_count += 1\n",
        "        print(f\"✅ Problem {i+1} passed all tests.\")\n",
        "    else:\n",
        "        print(f\"❌ Problem {i+1} failed some tests.\")\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = (passed_count / total_problems) * 100\n",
        "print(f\"\\nEvaluation Results:\")\n",
        "print(f\"Total problems: {total_problems}\")\n",
        "print(f\"Problems with all tests passed: {passed_count}\")\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "NwBRm9rfdxJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"gemma-3-mbpp-finetuned\")\n",
        "tokenizer.save_pretrained(\"gemma-3-mbpp-finetuned\")\n",
        "\n",
        "# Optionally save to GGUF format\n",
        "if False:  # Change to True to enable\n",
        "    model.save_pretrained_gguf(\n",
        "        \"gemma-3-mbpp-finetuned-gguf\",\n",
        "        quantization_type = \"Q8_0\",\n",
        "    )"
      ],
      "metadata": {
        "id": "Ml6b5PgfdxG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def chatbot():\n",
        "    print(\"Gemma-3 Code Assistant (type 'quit' to exit)\")\n",
        "    print(\"-------------------------------------------\")\n",
        "    print(\"Note: I will provide code solutions with explanations only.\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\nUser: \")\n",
        "        if user_input.lower() == 'quit':\n",
        "            break\n",
        "\n",
        "        # Format as a message with instruction to only provide code and explanation\n",
        "        messages = [{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [{\n",
        "                \"type\": \"text\",\n",
        "                \"text\": f\"{user_input}\\n\\nPlease provide only the Python code solution with a brief explanation. Do not include any test cases.\"\n",
        "            }]\n",
        "        }]\n",
        "\n",
        "        # Apply chat template\n",
        "        text = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            add_generation_prompt=True,\n",
        "        )\n",
        "\n",
        "        print(\"\\nAssistant:\")\n",
        "        streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "\n",
        "        # Generate response\n",
        "        _ = model.generate(\n",
        "            **tokenizer([text], return_tensors=\"pt\").to(\"cuda\"),\n",
        "            max_new_tokens=256,  # Reduced since we don't need test cases\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            top_k=50,\n",
        "            streamer=streamer,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            do_sample=True,\n",
        "            repetition_penalty=1.1,  # Helps avoid repeating test cases\n",
        "        )\n",
        "\n",
        "chatbot()"
      ],
      "metadata": {
        "id": "baVeCSejdxBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optional"
      ],
      "metadata": {
        "id": "TJfPBA2k2apG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16e4b849"
      },
      "source": [
        "!pip install Flask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1229f4c2"
      },
      "source": [
        "from flask import Flask\n",
        "\n",
        "app = Flask(__name__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d286c095"
      },
      "source": [
        "from flask import request, jsonify\n",
        "\n",
        "@app.route('/generate', methods=['POST'])\n",
        "def generate_code_api():\n",
        "    data = request.get_json()\n",
        "    prompt = data.get('prompt')\n",
        "\n",
        "    if not prompt:\n",
        "        return jsonify({\"error\": \"No prompt provided\"}), 400\n",
        "\n",
        "    # Format as a message with instruction to only provide code and explanation\n",
        "    messages = [{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [{\n",
        "            \"type\": \"text\",\n",
        "            \"text\": f\"{prompt}\\n\\nPlease provide only the Python code solution with a brief explanation. Do not include any test cases.\"\n",
        "        }]\n",
        "    }]\n",
        "\n",
        "    # Apply chat template\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "    )\n",
        "\n",
        "    # Generate response\n",
        "    outputs = model.generate(\n",
        "        **tokenizer([text], return_tensors=\"pt\").to(\"cuda\"),\n",
        "        max_new_tokens=256,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        top_k=50,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True,\n",
        "        repetition_penalty=1.1,\n",
        "    )\n",
        "\n",
        "    # Decode and extract the generated content\n",
        "    decoded_output = tokenizer.batch_decode(outputs)[0]\n",
        "\n",
        "    try:\n",
        "        # Attempt to find the start of the model's response\n",
        "        start_model_token = \"<start_of_turn>model\\n\"\n",
        "        start_index = decoded_output.find(start_model_token)\n",
        "        if start_index != -1:\n",
        "            start_index += len(start_model_token)\n",
        "            generated_content = decoded_output[start_index:].strip()\n",
        "\n",
        "            # Split to get code part before \"Test Cases:\" or other markers\n",
        "            test_cases_marker = \"\\n\\nTest Cases:\\n\"\n",
        "            explanation_marker = \"\\n\\nExplanation:\\n\" # Also look for Explanation marker\n",
        "            code_end_index_test = generated_content.find(test_cases_marker)\n",
        "            code_end_index_explanation = generated_content.find(explanation_marker)\n",
        "\n",
        "            if code_end_index_test != -1 and (code_end_index_explanation == -1 or code_end_index_test < code_end_index_explanation):\n",
        "                generated_code = generated_content[:code_end_index_test].strip()\n",
        "            elif code_end_index_explanation != -1:\n",
        "                 generated_code = generated_content[:code_end_index_explanation].strip()\n",
        "            else:\n",
        "                generated_code = generated_content.strip() # Take everything if no markers found\n",
        "\n",
        "        else:\n",
        "            generated_code = \"\" # No model response found\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting generated code: {e}\")\n",
        "        generated_code = \"\"\n",
        "\n",
        "    return jsonify({\"generated_code\": generated_code})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "529b5fa7"
      },
      "source": [
        "app.run(host='127.0.0.1', port=5000, debug=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecb84161"
      },
      "source": [
        "import threading\n",
        "import time\n",
        "\n",
        "def run_flask():\n",
        "    app.run(host='127.0.0.1', port=5000, debug=True, use_reloader=False)\n",
        "\n",
        "# Run Flask in a separate thread so the notebook is not blocked\n",
        "thread = threading.Thread(target=run_flask)\n",
        "thread.start()\n",
        "\n",
        "# Give the server a moment to start\n",
        "time.sleep(3)\n",
        "print(\"Flask server is running.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "099f11cf"
      },
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "url = 'http://127.0.0.1:5000/generate'\n",
        "prompt_data = {\"prompt\": \"write a function to calculate the factorial of a number\"}\n",
        "\n",
        "try:\n",
        "    response = requests.post(url, json=prompt_data)\n",
        "    response.raise_for_status()  # Raise an exception for bad status codes (4xx or 5xx)\n",
        "    generated_code = response.json().get('generated_code')\n",
        "\n",
        "    if generated_code:\n",
        "        print(\"Generated Code:\")\n",
        "        print(generated_code)\n",
        "    else:\n",
        "        print(\"No code generated.\")\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error sending request: {e}\")\n",
        "    print(\"Please ensure the Flask server is running in the background.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "215bb05b"
      },
      "source": [
        "# Attempt to kill the Flask process\n",
        "!pkill -f 'flask run'\n",
        "!pkill -f 'python run_flask.py' # In case it was run from a file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c167bae"
      },
      "source": [
        "# Regenerate and run the Flask app initialization and route definition\n",
        "from flask import Flask, request, jsonify\n",
        "import threading\n",
        "import time\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/generate', methods=['POST'])\n",
        "def generate_code_api():\n",
        "    data = request.get_json()\n",
        "    prompt = data.get('prompt')\n",
        "\n",
        "    if not prompt:\n",
        "        return jsonify({\"error\": \"No prompt provided\"}), 400\n",
        "\n",
        "    messages = [{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [{\n",
        "            \"type\": \"text\",\n",
        "            \"text\": f\"{prompt}\\n\\nPlease provide only the Python code solution with a brief explanation. Do not include any test cases.\"\n",
        "        }]\n",
        "    }]\n",
        "\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "    )\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **tokenizer([text], return_tensors=\"pt\").to(\"cuda\"),\n",
        "        max_new_tokens=256,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        top_k=50,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True,\n",
        "        repetition_penalty=1.1,\n",
        "    )\n",
        "\n",
        "    decoded_output = tokenizer.batch_decode(outputs)[0]\n",
        "\n",
        "    try:\n",
        "        start_model_token = \"<start_of_turn>model\\n\"\n",
        "        start_index = decoded_output.find(start_model_token)\n",
        "        if start_index != -1:\n",
        "            start_index += len(start_model_token)\n",
        "            generated_content = decoded_output[start_index:].strip()\n",
        "\n",
        "            test_cases_marker = \"\\n\\nTest Cases:\\n\"\n",
        "            explanation_marker = \"\\n\\nExplanation:\\n\"\n",
        "            code_end_index_test = generated_content.find(test_cases_marker)\n",
        "            code_end_index_explanation = generated_content.find(explanation_marker)\n",
        "\n",
        "            if code_end_index_test != -1 and (code_end_index_explanation == -1 or code_end_index_test < code_end_index_explanation):\n",
        "                generated_code = generated_content[:code_end_index_test].strip()\n",
        "            elif code_end_index_explanation != -1:\n",
        "                 generated_code = generated_content[:code_end_index_explanation].strip()\n",
        "            else:\n",
        "                generated_code = generated_content.strip()\n",
        "\n",
        "        else:\n",
        "            generated_code = \"\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting generated code: {e}\")\n",
        "        generated_code = \"\"\n",
        "\n",
        "    return jsonify({\"generated_code\": generated_code})\n",
        "\n",
        "def run_flask():\n",
        "    # Use a different port\n",
        "    app.run(host='127.0.0.1', port=5001, debug=True, use_reloader=False)\n",
        "\n",
        "# Run Flask in a separate thread\n",
        "thread = threading.Thread(target=run_flask)\n",
        "thread.start()\n",
        "\n",
        "# Give the server a moment to start\n",
        "time.sleep(3)\n",
        "print(\"Flask server is running on port 5001.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aecf57dc"
      },
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# Update the URL to the new port\n",
        "url = 'http://127.0.0.1:5001/generate'\n",
        "prompt_data = {\"prompt\": \"write a function to calculate the factorial of a number\"}\n",
        "\n",
        "try:\n",
        "    response = requests.post(url, json=prompt_data)\n",
        "    response.raise_for_status()\n",
        "    generated_code = response.json().get('generated_code')\n",
        "\n",
        "    if generated_code:\n",
        "        print(\"Generated Code:\")\n",
        "        print(generated_code)\n",
        "    else:\n",
        "        print(\"No code generated.\")\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error sending request: {e}\")\n",
        "    print(\"Please ensure the Flask server is running in the background on port 5001.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e43e879"
      },
      "source": [
        "!curl -X POST -H \"Content-Type: application/json\" -d '{\"prompt\": \"write a python function to check if a number is prime\"}' http://127.0.0.1:5001/generate"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}